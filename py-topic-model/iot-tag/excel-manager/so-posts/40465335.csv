thread_ID, Id, ParentId, PostType, Score, CreationDate, OwnerID, Body
40465335,40465335,null,1,3,Mon Nov 07 12:35:00 EST 2016,2256737,"<p>We plan to use Apache Flink with a huge IOT Setup. Customers are going to send us some kind of structured sensor data (like sensor_id, sensor_type, sensor_value, timestamp). We have no control when each customer sends this data, most likely in real-time, but we have no guarantee. We store all events in RabbitMQ/Kafka. UPDATE: We can assume that events per sensor come in order.</p>		<p>Before starting implementing a possible streaming pipeline, we are interested in solutions for the following challenges:</p>		<ol>	<li>Multiple Window Aggregations</li>	</ol>		<p>We store all raw sensor data into Cassandra. Further, we want to aggregate the sensor data by sensor_id on multiple time windows (e.g. 15 sek, 1 min, 15 min, 1 hour, 1 day). What is the recommended way to achieve the desired output efficiently with Flink streaming?</p>		<ol start="2">	<li>Very late data</li>	</ol>		<p>As already mentioned we have no control over <code>when</code> the data is sent. For example, a customer might experience network failures and therefore data might arrive late. How is the recommended way to handle this? How can we use watermarking if we can only guarantee good watermarks by sensor_id (because each customer has its own time/issues/failures)? We can add some allowed lateness (like 6 - 12 hours or so), is that managable with flinks in memory window storage? What happens after this allowed lateness? Should we store the really late data into another kafka topic and doing batch processing continuously? Lastly, some customers upload csv files with their collected sensor data. Does this guide as well to an batch approach?</p>		<ol start="3">	<li>Future data</li>	</ol>		<p>What happens to a stream, when some customer send us data that is far in the future, due to misconfigured sensors (as we have no control over it)?</p>		<p>We are curious about your recommendations. Thanks.</p>	"
40465335,40538821,40465335,2,3,Thu Nov 10 23:30:00 EST 2016,3609571,"<p>These are quite a few questions. I'll try to answer them one by one:</p>		<ol>	<li>Multiple window aggregations</li>	</ol>		<p>You can construct a data flow of cascading window operators and fork off (to emit or further processing) the result after each window.</p>		<pre><code>Input -&gt; window(15 secs) -&gt; window(1 min) -&gt; window(15 min) -&gt; ...	                        \-&gt; out_1        \-&gt; out_2         \-&gt; out_3	</code></pre>		<ol start="2">	<li>Very late data</li>	</ol>		<p>It seems the problem is rather that some data might arrive "very" late and not at the data is only in order per key. At the moment it is not possible to use per-key watermarks. So the "logical clock" is the same for all events. Flink's allowed lateness defines how long state is kept around to wait for late arriving data. If data arrives late (after a watermark) but within the allowed lateness, the corresponding state is still available and an update is computed. If an event is too late (later than allowed lateness) the state was discarded and the event is discarded as well. A high allowed lateness means that more state needs to be kept around. However, this issue can in principle be solved by scaling out. The processing of late data which goes into a dedicated Kafka topic can be done with Flink as well. Also periodic files can be better continuously processed with a stream processor. A batch solution needs to take care of data spanning files (externalizing state handling), scheduling of jobs, error handling, ...</p>		<ol start="3">	<li>Future data</li>	</ol>		<p>With Flink's watermark mechanism an operator always forwards its highest watermark (time cannot go backwards) but computes its watermark as the minimum watermark received from all input channels. So unless, you have future data on all channels, you should be fine. The future data would be put as state and will be computed when the time has reached "the future". That means, you won't lose data but you might have to wait for quite some time until it is processed.</p>		<hr>		<p>From your description, I would think about implementing the aggregation as a stateful FlatMap operator on a keyed stream. Given that the data per sensor arrives in order, you can do the necessary aggregations in a FlatMap (or a chain of FlatMaps, one for each time interval). </p>		<p>One challenge here would be that you do not know when to close an aggregate until you see an event that is later than the aggregation interval. In streams with globally valid watermarks, the time can advance (and windows be closed) even if not event for a specific key is received.</p>		<p>Another issue would be the removal of state in case a sensor is removed. This would not be automatically detected. Maybe a special marker record could be used to trigger the state clean-up.</p>	"
40465335,68324960,40538821,3,0,Fri Nov 11 07:33:00 EST 2016,2256737,"Q1: with the stateful flatMap would I need the winow function? How can I check if an element with key x arrived for window with end time slightly earlier than the events' time. Then I could purge the window."
40465335,68324968,40538821,3,0,Fri Nov 11 07:33:00 EST 2016,2256737,"Q2: Is there a possibility that flink can put very late data in a dedicated topic? We have periodic file per sensor, but all data would be put into the same kafka topic (sensor_data_csv). Therefore watermarking would be in our way as different file uploads span different time spans."
40465335,68324969,40538821,3,0,Fri Nov 11 07:33:00 EST 2016,2256737,"Q3: we have one single stream for all sensors thus the minimum of all inputs will be the maximum of this single stream, right? Could I start a timer and invalidate after each event, which fires after a long period to clean up really old windows when no data arrived for a long time?"
40465335,68324787,40538821,3,0,Fri Nov 11 07:26:00 EST 2016,2256737,"Thank, you very much for your comprehensive answer."
40465335,68328294,40538821,3,0,Fri Nov 11 09:28:00 EST 2016,3609571,"I think SO comments is not the right format for q&a's. I'd propose to go to the user mailing list for further questions (include the history for reference). Q1: If you use a FlatMapFunction you have to do the checks yourself. keyBy() and keyed state will help with that. Q2: Not yet, this is a [feature under discussion](https://cwiki.apache.org/confluence/display/FLINK/FLIP-13+Side+Outputs+in+Flink). Q3: If the single stream is processed in parallel, the min WM of all partitions is used. Support for timer in user functions is another upcoming feature. Not sure about the actual status of that."
