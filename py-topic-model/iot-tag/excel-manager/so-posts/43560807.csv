thread_ID, Id, ParentId, PostType, Score, CreationDate, OwnerID, Body
43560807,43560807,null,1,1,Sat Apr 22 15:10:00 EDT 2017,1802213,"<p>Running a sample application streaming data from kinesis. I did not get why this application uses so much heap and crashes.</p>		<p>Here is the code :</p>		<pre><code>from __future__ import print_function	import sys		from pyspark.streaming import StreamingContext	from pyspark.streaming.kinesis import KinesisUtils, InitialPositionInStream	from pyspark.sql.session import SparkSession	from datetime import datetime		# function declaration	def isDfEmpty(df):		    try:	        if not df.take(1) :	            return True		    except Exception as e:	        return True		    return False		# function declaration	def mergeTable(df):	    print("b:mergeTable")	    print(str(datetime.now()))		    try:	        global refDf		        if isDfEmpty(df) :	            print("no record, waiting !")	        else :	            if(isDfEmpty(refDf)) :	                refDf = df	            else :	                print("        before count %s" % refDf.count())	                refDf = df.unionAll(refDf)	                print("        after count %s" % refDf.count())		    except Exception as e:	        print(e)	    print(str(datetime.now()))	    print("e:mergeTable")   		# function declaration	def doWork(df):	    print("b:doWork")	    print(str(datetime.now()))		    try:	        mergeTable(df)	    except Exception as e:	        print(e)	    print(str(datetime.now()))	    print("e:doWork")		# function declaration	def sensorFilter(sensorType, rdd):	    df = spark.read.json(rdd.filter(lambda x : sensorType in x))	    doWork(df)			def printRecord(rdd):	    print("========================================================")	    print("Starting new RDD")	    print("========================================================")	    sensorFilter("SensorData", rdd)		refDf = None    		if __name__ == "__main__":	    reload(sys)  	    # sys.setdefaultencoding('utf-8')				    if len(sys.argv) != 5:	        print( "Usage: dump.py &lt;app-name&gt; &lt;stream-name&gt; &lt;endpoint-url&gt; &lt;region-name&gt;", file=sys.stderr)	        sys.exit(-1)		    spark = SparkSession.builder.master("local[*]").getOrCreate()	    sc = spark.sparkContext		    # sc = SparkContext(appName="PythonStreamingKinesisWordCountAsl")	    ssc = StreamingContext(sc, 10)	    appName, streamName, endpointUrl, regionName = sys.argv[1:]	    dstream = KinesisUtils.createStream(ssc, appName, streamName, endpointUrl, regionName, InitialPositionInStream.LATEST, 10)	    dstream.foreachRDD(printRecord)	    ssc.start()	    ssc.awaitTermination()	</code></pre>		<p>After a time the spark application slowed down due to heap usage. But when i comment out the lines, heap usage decrease to normal levels.(According to SparkUI)</p>		<pre><code>print("        before count %s" % refDf.count())	print("        after count %s" % refDf.count())	</code></pre>		<p>I am really new with pyspark and trying to get what is going on. 	Merging on data frame continuously may explode the memory of course but the problem of heap occurs very beginning. </p>		<p>EDIT	Environment : Tried on single ubuntu and on cents VM hosted by macOS nothing changed.</p>	"
43560807,74176830,43560807,3,0,Sat Apr 22 18:29:00 EDT 2017,6933215,"Action operations `count()` bring entire data to your driver node and you are repeatedly performing them again and again. Does that tell you anything? Take a look at how large `refDf`  can get and compare with your drive/local machine's capacity."
43560807,74237585,43560807,3,0,Mon Apr 24 17:05:00 EDT 2017,4865935,"Your batchinterval in StreamingContext must be in millis. If you want in seconds,  write Seconds(10) instead."
43560807,74177822,43560807,3,0,Sat Apr 22 19:11:00 EDT 2017,1802213,"I got your point @Pushkr as i mentioned on my note at the end. But be sure that it does not wait refDf to get as big as you expected. The heap explodes even really with small amounts."
