thread_ID, Id, ParentId, PostType, Score, CreationDate, OwnerID, Body
49576001,49576001,null,1,0,Fri Mar 30 14:10:00 EDT 2018,9575206,"<p>Our JavaScript program writing 20 messages asynchronously using kafka-rest every seconds.	 We try to do aggregation on incoming message but it return some inconsistent result.</p>		<p>Please find topic, stream and aggregated result table definition below.</p>		<p>Topic:</p>		<pre><code>./bin/kafka-avro-console-producer â€“broker-list localhost:9092 â€“topic order_flow â€“property value.schema='{â€œtypeâ€?:â€?recordâ€?,â€?nameâ€?:â€?myrecordâ€?,â€?fieldsâ€?:[{â€œnameâ€?:â€?OrderIDâ€?,â€?typeâ€?:â€?intâ€?},{â€œnameâ€?:â€?OrderDateâ€?,â€?typeâ€?:â€?longâ€?},{â€œnameâ€?:â€?Statusâ€?,â€?typeâ€?:â€?stringâ€?},{â€œnameâ€?:â€?ProductIDâ€?,â€?typeâ€?:â€?intâ€?}]}â€™	</code></pre>		<p>Stream:</p>		<pre><code>CREATE STREAM ORDERS_SRC WITH (KAFKA_TOPIC=â€™order_flowâ€™, VALUE_FORMAT=â€™AVROâ€™);	</code></pre>		<p>NEW STREAM â€“ this stream use the actual event date rather than time when message wrote in kafka.</p>		<pre><code>CREATE STREAM ORDERS WITH (TIMESTAMP =â€™ORDERDATEâ€™) AS SELECT ORDERDATE,ORDERID, STATUS, PRODUCTID FROM ORDERS_SRC;	</code></pre>		<p>Now we are aggregating data based on it status using:</p>		<pre><code>CREATE TABLE ORDERS_AGG_SEC as select Status,Count(*) from ORDERS_D WINDOW TUMBLING(SIZE 1 SECONDS) GROUP BY STATUS;	</code></pre>		<p>Now when we run query SELECT * FROM ORDERS_AGG_SEC; it returning below result</p>		<pre><code>1522328177000 | Processing : Window{start=1522328177000 end=-} | Processing | 20	1522328178000 | Processing : Window{start=1522328178000 end=-} | Processing | 20	1522328179000 | Processing : Window{start=1522328179000 end=-} | Processing | 5	1522328179000 | Processing : Window{start=1522328179000 end=-} | Processing | 20	1522328180000 | Processing : Window{start=1522328180000 end=-} | Processing | 20	1522328181000 | Processing : Window{start=1522328181000 end=-} | Processing | 15	1522328181000 | Processing : Window{start=1522328181000 end=-} | Processing | 20	1522328182000 | Processing : Window{start=1522328182000 end=-} | Processing | 20	1522328183000 | Processing : Window{start=1522328183000 end=-} | Processing | 15	1522328183000 | Processing : Window{start=1522328183000 end=-} | Processing | 20	1522328184000 | Processing : Window{start=1522328184000 end=-} | Processing | 20	1522328185000 | Processing : Window{start=1522328185000 end=-} | Processing | 15	1522328185000 | Processing : Window{start=1522328185000 end=-} | Processing | 20	1522328186000 | Processing : Window{start=1522328186000 end=-} | Processing | 20	1522328187000 | Processing : Window{start=1522328187000 end=-} | Processing | 15	1522328187000 | Processing : Window{start=1522328187000 end=-} | Processing | 20	1522328188000 | Processing : Window{start=1522328188000 end=-} | Processing | 20	1522328189000 | Processing : Window{start=1522328189000 end=-} | Processing | 15	1522328189000 | Processing : Window{start=1522328189000 end=-} | Processing | 20	1522328190000 | Processing : Window{start=1522328190000 end=-} | Processing | 20	1522328191000 | Processing : Window{start=1522328191000 end=-} | Processing | 15	</code></pre>		<p>Expected Result:	I should get 20 count at every 1 second for Processing status</p>		<p>Actual Result:	I am getting more than one records for every 1 second interval for same status like below:</p>		<pre><code>1522328179000 | Processing : Window{start=1522328179000 end=-} | Processing | 5	1522328179000 | Processing : Window{start=1522328179000 end=-} | Processing | 20	</code></pre>		<p>Please find my javascript code below:</p>		<pre class="lang-js prettyprint-override"><code>function getRandomInt(min, max) {	    min = Math.ceil(min);	    max = Math.floor(max);	    return Math.floor(Math.random() * (max â€“ min)) + min; //The maximum is exclusive and the minimum is inclusive	}	var orderdate = Date.now();	for (var i = 0; i &lt; 20; i++) {	    var data = {	        "OrderID": getRandomInt(1, 20000),	        "OrderDate": orderdate,	        "Status": "Processing",	        "ProductID": getRandomInt(1, 10)	    }	    node.send({payload:data}); // this function asynchronously call kafka-rest api producer.	}	</code></pre>		<p>Note: kafka rest api running with default properties</p>	"
49576001,49614646,49576001,2,4,Mon Apr 02 16:07:00 EDT 2018,6727769,"<p>KSQL uses Kafka Streams to run queries and the behavior you described is the one expected for aggregate results. Every time a new record arrives the query will execute and the updated results for the corresponding record will be emitted. You can configure how often the results are emitted by setting <code>commit.interval.ms</code> and <code>buffered.records.per.partition</code>.	The result you see in the output of your aggregate query indeed are the latest results up to that point. For instance </p>		<p><code>1522328179000 | Processing : Window{start=1522328179000 end=-} | Processing | 5</code></p>		<p>means that the count is 5 up to now (this is a partial result) and when you receive more records for the same group and window the count will be updated and a more up to date results will be emitted :</p>		<p><code>1522328179000 | Processing : Window{start=1522328179000 end=-} | Processing | 20</code></p>		<p>This means that the count is updated to 20 now.	If you want to have the most up to date results, you can materialize the results in a table. This way, the table will always have the most up to date counts for each group.</p>	"
49576001,86311409,49614646,3,0,Wed Apr 04 12:57:00 EDT 2018,9575206,"Thank you for the reply,   but how can i create a materialized view in KSQL ?  can you guide me and i did use setting  'buffered.records.per.partition' but in my case where every 100ms i am getting 20 records what is the ideal records per partition i should keep ?"
49576001,86407796,49614646,3,0,Fri Apr 06 16:43:00 EDT 2018,6727769,"The results of your query are written into a kafka topic. You can use Kafka Connect to materialize the topic in an external store such as a Key/Value store or a RDBMS. You can use JDBC sink connector for RDBMS systems such as mysql or postgres (https://docs.confluent.io/current/connect/connect-jdbc/docs/sink_connector.html) or Cassandra sink (https://www.confluent.io/blog/kafka-connect-cassandra-sink-the-perfect-match/). The table in the RDBMS or Key/Value store will always have the latest result and only one value per key."
