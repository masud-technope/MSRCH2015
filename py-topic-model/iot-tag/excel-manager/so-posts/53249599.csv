thread_ID, Id, ParentId, PostType, Score, CreationDate, OwnerID, Body
53249599,53249599,null,1,1,Sun Nov 11 14:15:00 EST 2018,2701191,"<p>Iâ€™m currently working on a Raspberry Pi/Django project slightly more complex that iâ€™m used to. (i either do local raspberry pi projects, or simple Django websites; never the two combined!)</p>		<p>The idea is two have two Raspberry Piâ€™s collecting information running a local Python script, that would each take input from one HDMI feed (iâ€™ve got all that part figured out - I THINK) using image processing. Now i want these two Raspberry Piâ€™s (that donâ€™t talk to each other) to connect to a backend server that would combine, store (and process) the information gathered by my two Pis</p>		<p>Iâ€™m expecting each Pi to be working on one frame per second, comparing it to the frame a second earlier (only a few different things he is looking out for) isolate any new event, and send it to the server. Iâ€™m therefore expecting no more than a dozen binary timestamped data points per second.</p>		<p>Now what is the smart way to do it here ? </p>		<ul>	<li>Do i make contact to the backend every second? Every 10 seconds?</li>	<li>How do i make these bulk HttpRequests ? Through a POST request? Through a simple text file that i send for the Django backend to process? (i have found some info about â€œbulk updatesâ€? for django but iâ€™m not sure that covers it entirely)</li>	<li>How do i make it robust? How do i make sure that all data what successfully transmitted before deleting the log locally ? (if one call fails for a reason, or gets delayed, how do i make sure that the next one compensates for lost info?</li>	</ul>		<p><strong>Basically, iâ€™m asking advise for making a IOT based project, where a sensor gathers bulk information and want to send it to a backend server for processing, and how should that archiving process be designed.</strong></p>		<p>PS: i expect the image processing part (at one fps) to be fast enough on my Pi Zero (as it is VERY simple); backlog at that level shouldnâ€™t be an issue.</p>		<p>PPS: iâ€™m using a django backend (even if it seems a little overkill) 	    a/ because i already know the framework pretty well	    b/ because iâ€™m expecting to build real-time performance indicators from the combined data points gathered, using django, and displaying them in (almost) real-time on a webpage.</p>		<p>Thank you very much !</p>	"
53249599,93384387,53249599,3,0,Sun Nov 11 14:37:00 EST 2018,104349,"How do you get from "two clients working on one frame per second" to "a dozen data points per second"? Does each frame produce several data points?"
53249599,93384460,53249599,3,0,Sun Nov 11 14:41:00 EST 2018,2701191,"yeah; it looks at different parts of the image that may or may not have text appear in them (always at the same spot!)"
53249599,93384771,53249599,3,0,Sun Nov 11 14:58:00 EST 2018,3929826,"Process one frame, collect the data form it, send it and handle the exception if one was raised."
53249599,53249964,53249599,2,2,Sun Nov 11 14:58:00 EST 2018,104349,"<p>This partly depends on just how resilient you need it to be. If you really can't afford for a single update to be lost, I would consider using a message queue such as RabbitMQ - the clients would add things directly to the queue and the server would pop them off in turn, with no need to involve HTTP requests at all.</p>		<p>Otherwise it would be much simpler to just POST each frame's data in some serialized format (ie JSON) and Django would simply deserialize and iterate through the list, saving each entry to the db. This <em>should</em> be fast enough for the rate you describe - I'd expect saving a dozen db entries to take significantly less than half a second - but this still leaves the problem of what to do if things get hung up for some reason. Setting a super-short timeout on the server will help, as would keeping the data to be posted until you have confirmation that it has been saved - and creating unique IDs in the client to ensure that the request is idempotent. </p>	"
