thread_ID, Id, ParentId, PostType, Score, CreationDate, OwnerID, Body
40670755,40670755,null,1,0,Fri Nov 18 06:34:00 EST 2016,1000509,"<p>I am working for a M2M IoT provider. There are millions of deployed IoT devices in the field. The data is aggregated by edge devices and sent to our central server. Server processes the device data and sends out the processed info to various other sub-systems.</p>		<p>We are conceptualizing the idea of an analytics engine (AE) in parallel to existing system. The data coming from the IoT devices shall be fed into this engine to come with different types of analysis. Example: Monitor the events coming from a device over the last 24 hours and figure out the health or predict other stuff.	We are trying to figure out the answers to the below questions in this regard:</p>		<p>1) Where shall we tap the incoming data from IoT devices to feed into the new system(AE)? If we tap at the existing server then we shall be introducing a strong coupling between the two systems. Any downtime at the exiting server shall dry the pipeline to AE as well. What is the general recommended strategy for such a case?</p>		<p>2) We plan to front end AE with Kafka to ensure data availability. There shall be various micro services acting as Kafka consumers and doing their stuff. What shall be the architecture of a micro service which shall manage device state as per the events reported during the last 24 hours (i.e. need to consider both historical and live data)? There are numerous technologies out there like Spark, Hadoop, Apache storm and I am not sure which one to start with. How should I persist the incoming device data and how to process historical and incoming data together to figure out current device state? What is the standard practice in such scenarios: Is the summarized data calculated and stored beforehand or is it calculated in real time as per the incoming query?</p>	"
40670755,40695779,40670755,2,0,Sat Nov 19 17:21:00 EST 2016,1870387,"<p>General: as the number of devices (Millions) is less relevant, I use the term tps (transaction per second) to refer to the load.</p>		<p>On 1): You must tap-in at some place. :-) In our IoT platform we tap-in directly after inbound data processing and forward to our AE. In any case, ensure that the inbound AE capacity (in terms of tps) is higher then the one of "the server".</p>		<p>IMHO its less a question about coupling. Its more about responsibility, i.e. which component is responsible to "ack" the data transfer, storing it reliable, etc. </p>		<p>On 2) It depends. Especially it depends on </p>		<ol>	<li>what algorithms you use in analytics</li>	<li>the real-time behaviour do your customer expect (show result in seconds vs. show results every 24h)</li>	<li>the reliability requirements (can you afford to loose some results in case of server failures)</li>	<li>the data amounts (how many tps, also on a per-device level?)</li>	<li>are the analytics algorithms "device local" vs. you mix data of many devices?</li>	</ol>		<p>Depending on your answers, you might want to have a look a t CEP engines (Complex event processing) as a basis for your AE (Analytical Engine)</p>	"
