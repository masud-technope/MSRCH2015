thread_ID, Id, ParentId, PostType, Score, CreationDate, OwnerID, Body
44810821,44810821,null,1,1,Wed Jun 28 19:16:00 EDT 2017,3046674,"<p>I really want to get an architectural solution for my below scenario.</p>		<p>I have a source of events (Say sensors in oil wells , around 50000 ), that produces events to a server. At the server side I want to process all these events in such a way that , the information from the sensors about latest humidity, temperature,pressure ...etc will be stored/updated to a database.</p>		<p>I am confused with flume or kafka. </p>		<p>Can somebody please address my simple scenario in architectural terms. </p>		<p>I don't want to store the event somewhere, since I am already updating the database with latest values. </p>		<p>Should I really need spark , (flume/kafka) + spark , to meet the processing side?.</p>		<p>Can we do any kind of processing using flume without a sink?</p>	"
44810821,44815754,44810821,2,0,Thu Jun 29 03:09:00 EDT 2017,1433596,"<p>One way would be to push all the messages to Kafka Topic. Using Spark Stream you can ingest and process from the kafka topic. Spark streaming can directly process from your Kafka Topic</p>	"
44810821,76616353,44815754,3,0,Thu Jun 29 06:52:00 EDT 2017,4950528,"Yes or Kafka Streams could do the processing right within Kafka and you would not need Spark. It all depends on what kind of "processing" needs to be done."
44810821,76676380,44815754,3,0,Fri Jun 30 13:43:00 EDT 2017,3046674,"The processing is performed under Kafka consumer API?"
44810821,44816201,44810821,2,1,Thu Jun 29 04:08:00 EDT 2017,2188893,"<p>Sounds like you need to use the Kafka producer API to publish the events to a topic then simply read those events either by using the Kafka consumer API to write to your database or use the Kafka JDBC sink connector.</p>		<p>Also if you need just the latest data inside Kafka take a look at log compaction.</p>	"
