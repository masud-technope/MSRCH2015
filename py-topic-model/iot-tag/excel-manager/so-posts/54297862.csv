thread_ID, Id, ParentId, PostType, Score, CreationDate, OwnerID, Body
54297862,54297862,null,1,0,Mon Jan 21 21:14:00 EST 2019,10917405,"<p>Hi everyone Im having the following issue with Google Cloud IoT specificly with the Registry Creation and subfolders:</p>		<p>I have a device with exactly 19 values to send and all are important now google let me create a registry with upto 10topics/subfolders</p>		<p>the original idea [<strong>IDEA</strong>] was to use a structure like so:</p>		<ul>	<li>topics/PowerMeter/PM1/v1</li>	<li>topics/PowerMeter/PM1/V2</li>	<li>topics/PowerMeter/PM1/v3</li>	<li>topics/PowerMeter/PM1/C1</li>	<li>topics/PowerMeter/PM1/C2</li>	<li>topics/PowerMeter/PM1/C3</li>	<li>....and so on for 19 values of /PM1/</li>	</ul>		<p>the question... is it a good practice to send a bunch of values packed in a payload to then process in the cloud? or is it best to have all separated in topics?...</p>		<p>i understand that cloud functions gets more expensive as the time elapsed to acomplish the task is greater, so maybe separate in topics should be ok?</p>		<p>if so, how to do it? there is just 10 topics/subfolders to work with....</p>		<p>then how to handle alarms with the same subfolders? i need some kind of guidance about this little proyect maybe google documentation confused me?</p>		<p>anyway <strong>anyone</strong> your help is very very appretiated</p>	"
54297862,95838708,54297862,3,0,Mon Feb 04 14:12:00 EST 2019,10917405,"**GabeWeiss** i want to only store this inside tables maybe two max. the thing is the device im reading needs to save exactly 20 variables, maybe take all those variables in a json and then process and store with cloud functions? would it be fast - best practice this way? separate in 20 topics doesnt seems possible......"
54297862,95773834,54297862,3,0,Fri Feb 01 16:25:00 EST 2019,3534690,"What's the end-goal of the data you have coming in? Will it all get stored in the same database? Real-time analysis only? While functions do get more expensive with more run-time, they also get more expensive with # of executions. So depending on how many we're talking about, it could be an issue there as well. Broadly speaking, unless you're doing incredibly complicated things with the results from the data, it's going to be fast anyway so the time elapsed (CPU usage) won't really be drastically more than multiple-invocations with shorter CPU time."
54297862,54524946,54297862,2,1,Mon Feb 04 22:00:00 EST 2019,3534690,"<p>Now that I've gotten clarity from the comment, I'd definitely have all the values in one json blob, use a single cloud function to parse and move to storage. The amount of process time to handle 19 values vs 1 value each is going to be much smaller compared to spinning up 19/20 individual functions for sure. Even if there's a good amount of work that's done with the data, you're likely paying this cost in each individual function anyway. From an infrastructure standpoint, managing one vs 20 will be much easier/better as well.</p>	"
54297862,95882522,54524946,3,0,Tue Feb 05 18:17:00 EST 2019,3534690,"No problem! If you have problems, feel free to reach out to me on Twitter, my DMs are open (@GabeWeiss_)"
54297862,95873253,54524946,3,0,Tue Feb 05 13:49:00 EST 2019,10917405,"Thank you so much @GabeWeiss		'From an infrastructure standpoint, managing one vs 20 will be much easier/better as well' this was the main concern"
