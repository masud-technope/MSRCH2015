thread_ID, Id, ParentId, PostType, Score, CreationDate, OwnerID, Body
47770122,47770122,null,1,0,Tue Dec 12 10:18:00 EST 2017,3298327,"<p>I using watches and implementing some custom IoT logic. I'm using netty to receive connection from watch and store some data and everyting working fine with that approach. But I got new requirement to send some command to watch. In order to do that I can send command to already opened channel on Netty but with that approach I need to store somewhere map with <strong>imei</strong> and opened <strong>netty channel</strong>. Are there some solutions for that? Or probably this approach is fine...</p>	"
47770122,47770754,47770122,2,1,Tue Dec 12 10:49:00 EST 2017,7110799,"<p>Netty has <a href="https://netty.io/4.0/api/io/netty/channel/group/DefaultChannelGroup.html" rel="nofollow noreferrer">DefaultChannelGroup</a> for that. Also, you may create own <code>ConcurrentHashMap</code> to hold required channels. Your question is not very clear, so It is hard to give the more precise answer.</p>	"
47770122,82502633,47770754,3,0,Tue Dec 12 10:56:00 EST 2017,3298327,"Ok so let me give you some background. I have server netty server which handles connections from watches and it works pretty well when we need to handle only incoming connections. But we got new requirement to be able to send some data to watch. So currently we don't have another way than store map with imei and opened channel to retrieve that channel if we have got message for watch and then send this messege using that opened channel. So my question is: are there any existing solutions for that? Because having hashmap on netty will prevent us from horizontal scaling..."
47770122,82503416,47770754,3,0,Tue Dec 12 11:18:00 EST 2017,3298327,"is it more clear now?"
47770122,82527740,47770754,3,0,Tue Dec 12 23:23:00 EST 2017,7110799,"Your approach is totally fine. Scaling the tcp connections is another problem and has own solutions, like client side load balancing."
47770122,82540110,47770754,3,1,Wed Dec 13 09:27:00 EST 2017,3298327,"Got it - thanks for your answer!"
