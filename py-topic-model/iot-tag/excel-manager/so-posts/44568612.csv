thread_ID, Id, ParentId, PostType, Score, CreationDate, OwnerID, Body
44568612,44568612,null,1,1,Thu Jun 15 13:18:00 EDT 2017,8137842,"<p>I was concerned about real time stream processing for IOT through GCD pub/sub, Cloud Dataflow and perform analytics through BigQuery.I am seeking help for how to implement this.	<a href="https://i.stack.imgur.com/7RhEw.png" rel="nofollow noreferrer">Here is the architecture for IOT real-time stream processing</a></p>	"
44568612,44578412,44568612,2,1,Thu Jun 15 23:15:00 EDT 2017,1797836,"<p>I'm assuming you mean that you want to stream some sort of data from outside the Google Cloud Platform into BigQuery.</p>		<p>Unless you're transforming the data somehow, I don't think that Data Flow is necessary.</p>		<p>Note, that BigQuery has its own Streaming API so you don't necessarily have to use Pub/Sub to get data into BigQuery.</p>		<p>In any case, these are the steps you should generally follow.</p>		<h1>Method 1</h1>		<ol>	<li>Issue a service account (and download the .json file from IAM on Google Console)</li>	<li>Write your application to get the data you want to stream in</li>	<li>Inside that application, use the service account to stream directly into a BQ dataset and table</li>	<li>Analyse the data on the BigQuery console (<a href="https://bigquery.cloud.google.com" rel="nofollow noreferrer">https://bigquery.cloud.google.com</a>)</li>	</ol>		<h1>Method 2</h1>		<ol>	<li>Setup PubSub queue</li>	<li>Write an application that collections the information you want to stream in</li>	<li>Push to PubSub</li>	<li>Configure DataFlow to pull from PubSub, transform the data however you need to and push to BigQuery</li>	<li>Analyse the data on the BigQuery console as above.</li>	</ol>		<h2>Raw Data</h2>		<p>If you just want to put very raw data (no processing) into BQ, then I'd suggest using the first method.</p>		<h2>Semi Processed / Processed Data</h2>		<p>If you actually want to transform the data somehow, then I'd use the second method as it allows you to massage the data first.</p>		<h3>Try to always use Method 1</h3>		<p>However, I'd usually always recommend using the first method, even if you want to transform the data somehow. </p>		<p>That way, you have a <code>data_dump</code> table (raw data) in your dataset and you can still use DataFlow after that to transform the data and put it <strong><em>back</em></strong> into an <code>aggregated</code> table. </p>		<p>This gives you maximum flexibility because it allows you to create potentially <code>n</code> transformed datasets from the single <code>data_dump</code> table in BQ.</p>	"
