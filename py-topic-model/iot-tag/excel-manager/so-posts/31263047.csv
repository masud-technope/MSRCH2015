thread_ID, Id, ParentId, PostType, Score, CreationDate, OwnerID, Body
31263047,31263047,null,1,2,Tue Jul 07 08:05:00 EDT 2015,1948655,"<p>I am creating a program where I need to talk to multiple devices (in the order of 10-20 devices), over different physical connections (serial port, UDP). These devices only reply to requests I do to them, and each of them only process one request before allowing a new one. The application might request a value update from each of them every second.</p>		<p>As of now, I have an interface <code>IRequestReplyDevice</code></p>		<pre><code>public interface IRequestReplyDevice	{	    T SendMessage&lt;T&gt;(IMessage message) where T : IMessage;	}	</code></pre>		<p>Where SendMessage is a blocking call that returns the response received from the device. In each implementation of this interface, ex. <code>SerialPortDevice : IRequestReplyDevice</code>, I have a lock in <code>SendMessage</code> that ensures that no new message is sent until the response to the previous reply is received and returned to caller.</p>		<p>I was wanting to build a Web API on top of this, and that may lead to several clients wanting to request something from the same device at the same time.</p>		<p>Is this approach robust or even sane? Would you approach it differently? </p>	"
31263047,31273692,31263047,2,3,Tue Jul 07 15:59:00 EDT 2015,3673379,"<p>Based on the above, my initial thought would be to remove the blocking calls and instead decouple the request and response chain with queues if it is possible.</p>		<p>The flow would be similar to the below</p>		<p><em>Request -> RequestQueue -> RequestHandler -> ResponseQueue -> ResponseHandler</em></p>		<p>The rationale behind this recommendation is that blocking calls and the inherent concurrency of multiple users will lead to a lot of complex locking, will have an inherent bottleneck at the lock and may not scale well.</p>		<p>The issue with this solution however is that it will involve a good amount of extra work and moving parts.  This leads to the real question of what behaviour do you <strong><em>actually</em></strong> need from the system?  Designing a system that requires high-throughput (1mb/s?  1gb/s?) and low-latency (under 100ms?  under 3ms?) that handles concurrency well can get very complicated very quickly.</p>		<p>If the system can tolerate the latency, throughput and scale requirements behind a simple block / lock design than by all means use it.  If you have tested the performance of a lock based architecture under load and it doesn't perform adequately or if you have the reasonable expectation that the system will grow to  the point where it will fail to meet requirements in the near future than I would definitely recommend looking at using queues.</p>	"
31263047,50590854,31273692,3,0,Wed Jul 08 18:03:00 EDT 2015,3673379,"You're welcome. Walkingsteak.  The easiest way in my opinion would be to use a WCF service that handles the request - response infrastructure plumbing (aka monkeycode) but that again depends upon actual needs.  Without understanding more of the actual system requirements (FURPS+) it is difficult to provide recommendations that will perform adequately in the real-world."
31263047,50581101,31273692,3,0,Wed Jul 08 14:11:00 EDT 2015,1948655,"Thanks for the suggestion! How would you go about returning a given response to the caller who sent the request, for example returning a HTTP response to a HTTP request."
