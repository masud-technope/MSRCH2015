thread_ID, Id, ParentId, PostType, Score, CreationDate, OwnerID, Body
33517932,33517932,null,1,2,Wed Nov 04 09:20:00 EST 2015,1602831,"<p>I'm trying to design an architecture of my streaming application and choose the right tools for the job.</p>		<p>This is how it works currently:	<img src="https://photos-3.dropbox.com/t/2/AABY7uSjmrbcplaj4b8M387lHyaxEaXUDp1P5WpZWKRYYg/12/27579084/png/32x32/1/_/1/2/spark_streaming.png/EKK94RQY95YDIAIoAg/s2lUoNfSkN9g4rcL4S8x9MG-iNT-HHz59YcfHP0wbwE?size=1024x768&amp;size_mode=2" alt="app-arch"></p>		<p>Messages from "application-producer" part have a form of <code>(address_of_sensor, timestamp, content)</code> tuples. </p>		<p>I've already implemented all functionality before Kafka, and now I've encountered major flaw in the design. In "Spark Streaming" part, consolidated stream of messages is translated into stream of events. The problem is that events for the most part are composite - consist of multiple messages, which have occurred at the same time at different sensors. </p>		<p>I can't rely on "time of arrival to Kafka" as a mean to detect "simultaneity". So I has to somehow sort messages in Kafka before extracting them with Spark. Or, more precisely, make queries over Kafka messages. </p>		<p>Maybe Cassandra is the right replacement for Kafka here? I have really simple data model, and only two possible types of queries to perform: query by address, and range query by timestamp. Maybe this is the right choice? </p>		<p>Do somebody have any numbers of Cassandra's throughput?  </p>	"
33517932,33518705,33517932,2,0,Wed Nov 04 09:55:00 EST 2015,535024,"<p>If you want to run queries on your time series, Cassandra may be the best fit - it is very write optimized, you can build 'wide' rows for your series. It is possible to make slices on your wide rows, so you can select some time ranges with only one query.</p>		<p>On the other hand, kafka can be considered as a raw data flow - you don't have queries, only recently produced data. In order to collect data based on some key in the same partition, you have to select this key carefully. All data within same partition are time sorted.</p>	"
33517932,54821640,33518705,3,0,Wed Nov 04 11:02:00 EST 2015,1602831,"What if I create a lot of topics, where topic=address in Kafka? Will it work? Then I can make some assumption about "incorrectness" of arrival time compare to value of `timestamp`, and give up of address query at all."
33517932,54824890,33518705,3,0,Wed Nov 04 12:25:00 EST 2015,535024,"@mkurnikov a lot of topics is probably not a good idea. But you can create a topic for your data and choose the address field as key. In that case all events with the same address go to the same partition and are time sorted. You can even create a topic with single partition and store all events in that partition. In this case you get all events ordered, but the throughput will be bad."
33517932,33519063,33517932,2,0,Wed Nov 04 10:12:00 EST 2015,4656721,"<p>Range Query on Timestamp is the classic use case of cassandra , if u need address based queries as well u would have to make them as clustering column if using cassandra . As far as cassandra througput are concerned if you can invest in proper performance analysis on cassandra cluster you can achieve very high write throughput . But I have used SparkQL , Cassandra Driver and spark Cassandra connector they don't really give high query throughput speed until you have a big cluster with high CPU configuration , it does not work well with small dataset .</p>		<p>Kafka should not be used as data source for queries , its more of commit log</p>	"
33517932,54821730,33519063,3,0,Wed Nov 04 11:04:00 EST 2015,1602831,"Should I clean used messages from Cassandra? It should affect performance too.  In case of Kafka, AFAIK, it really doesn't matter how much data is in the queue, because of underlying data model."
33517932,54874251,33519063,3,0,Thu Nov 05 14:36:00 EST 2015,4656721,"IF you clean used messages .. it created tombstones and tombstones create high cpu usage during compaction ..  so i would't suggest data deletion or cleaning algos with cassandra ."
