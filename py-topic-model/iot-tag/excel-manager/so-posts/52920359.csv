thread_ID, Id, ParentId, PostType, Score, CreationDate, OwnerID, Body
52920359,52920359,null,1,1,Sun Oct 21 22:10:00 EDT 2018,10537767,"<p>I am building an IoT device that will be producing 200Kb of data per second, and I need to save this data to storage. I currently have about 500 devices, I am trying to figure out what is the best way to store the data? And the best database for this purpose? In the past I have stored data to GCP's BigQuery and done processing by using compute engine instance groups, but the size of the data was much smaller.</p>	"
52920359,52920679,52920359,2,1,Sun Oct 21 23:08:00 EDT 2018,8016720,"<p>This is my best answer based upon the limited information in your question. </p>		<p>The first step is to document / describe what type of data that you are processing. Is it structured data (SQL) or unstructured (NoSQL)? What type of queries do you need to make? How long do you need to store the data and what is the expected total data size. This will determine the choice of the backend performing the query processing and analytics.</p>		<p>Next you need to look at the rate of data being transmitted. At 200 Kbits (or is it 200 KBytes) times 500 devices this is 100 Mbits (or 800 MBits) per second. How valuable is the data and how tolerant is your design for data loss? What is the data transfer rate for each device (cellular, wireless, etc.) and connection reliability?.</p>		<p>To push the data into the cloud I would use Pub/Sub. Then process the data to merge, combine, compress, purge, etc and push to Google Cloud Storage or to BigQuery (but other options may be better such as Cloud SQL or Cloud Datastore / BigTable). The answer for the intermediate processor depends on the previous questions but you will need some horsepower to process that rate of data stream. Options might be Google Cloud Dataproc running Spark or Google Cloud Dataflow.</p>		<p>There is a lot to consider for this type of design. My answer has created a bunch of questions, hopefully this will help you architect a suitable solution.</p>	"
52920359,92751371,52920679,3,0,Mon Oct 22 00:12:00 EDT 2018,10537767,"Thank you so much for your long and thoughtful answer! To answer your questions: 100 Mbits streamed through TCP load balancer on GCP through wifi, and we assume good connection reliability. 	To combine your suggestions, it seems a good way to process the data would probably be Device -> TCP balancer ->Pub/Sub->Dataproc or Dataflow-> Cloud Datastore/BigTable or Cloud SQL.	I will do more research on the options you suggested, but please let me know if the flow from above is problematic!"
52920359,92753389,52920679,3,0,Mon Oct 22 03:33:00 EDT 2018,8016720,"The only problem that I see is "Device -> TCP balancer -> Pub/Sub". I am not aware of any load balancers that will know how to package messages for Pub/Sub. I would write directly from Device -> PubSub. By using Pub/Sub you have a highly available fault tolerant entry point for your data stream packets. This will also decouple performance dependencies."
52920359,92755758,52920679,3,0,Mon Oct 22 06:16:00 EDT 2018,10537767,"Sorry I forgot to include the GCP group instance, so it would be Device -> TCP balancer -> instance group -> Pub/Sub. Since the hardware doesn't have the capability to directly us Pub/Sub yet."
52920359,52972631,52920359,2,0,Wed Oct 24 15:20:00 EDT 2018,3534690,"<p>You could also look at IoT Core as a possible way to handle the load balancing piece (it auto-scales). There would be some up front overhead registering all your devices, but it also then handles secure connection as well (TLS stack + JWT encryption for security on devices using IoT Core).</p>		<p>With 500 devices and 200KB/s, that sound well within the capabilities of the system to handle. Pub/Sub is the limiter, and it handles between 1-2M messages per second so it should be fine.</p>	"
