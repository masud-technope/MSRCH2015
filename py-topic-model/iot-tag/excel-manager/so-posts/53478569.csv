thread_ID, Id, ParentId, PostType, Score, CreationDate, OwnerID, Body
53478569,53478569,null,1,1,Mon Nov 26 09:55:00 EST 2018,2938258,"<p>We are working on HomeKit-enabled IoT devices. HomeKit is designed for consumer use and does not have the ability to collect metrics (power, temperature, etc.), so we need to implement it separately.</p>		<p>Let's say we have 10 000 devices. They send one collection of metrics every 5 seconds. So each second we need to receive 10000/5=2000 collections. The end-user needs to see graphs of each metric in the specified period of time (1 week, month, year, etc.). So each day the system will receive 172,8 millions of records. Here come a lot of questions.</p>		<p>First of all, there's no need to store all data, as the user needs only graphs of the specified period, so it needs some aggregation. What database solution fits it? I believe no RDMS will handle such amount of data. Then, how to get average data of metrics to present it to the end-user?</p>		<p>AWS has shared time-series data processing architecture:	<a href="https://i.stack.imgur.com/p1zVe.jpg" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/p1zVe.jpg" alt="enter image description here"></a></p>		<p>Very simplified I think of it this way:</p>		<ol>	<li>Devices push data directly to DynamoDB using HTTP API</li>	<li>Metrics are stored in one table per 24 hours</li>	<li>At the end of the day some procedure runs on Elastic Map Reduce and	produces ready JSON files with data required to show graphs per time	period.</li>	<li>Old tables are stored in RedShift for further applications.</li>	</ol>		<p>Has anyone already done something similar before? Maybe there is simpler architecture?</p>	"
53478569,53567717,53478569,2,0,Sat Dec 01 04:14:00 EST 2018,1874487,"<p>This requires bigdata infrastructure like 	1) Hadoop cluster	2) Spark 	3) HDFS 	4) HBase</p>		<p>You can use Spark to read the data as stream. The steamed data can be store in HDFS file system that allows you to store large file across the Hadoop cluster. You can use map reduce algorithm to get the required data set from HDFS and store in HBase which is the Hadoop database. HDFS is  distributed, scalable and big data store to store the records. Finally, you can use the query tools to query the hbase.</p>		<p><strong>IOT data --> Spark --> HDFS --> Map/Reduce --> HBase -- > Query Hbase.</strong></p>		<p>The reason I am suggesting this architecture is for 	<strong>scalability</strong>. The input data can grow based on the number of IOT devices. In the above architecture, infrastructure is distributed and the nodes in the cluster can grow without limit. </p>		<p>This is proven architecture in big data analytics application.</p>	"
