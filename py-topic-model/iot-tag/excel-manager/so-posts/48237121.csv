thread_ID, Id, ParentId, PostType, Score, CreationDate, OwnerID, Body
48237121,48237121,null,1,0,Sat Jan 13 05:14:00 EST 2018,122441,"<p>I'm in an <a href="https://github.com/lskk/ecnweb" rel="nofollow noreferrer">academic research project</a>, and using MongoDB to store time series data for accelerometer values (IoT/telemetry data). The granularity is samples where sample rate can be anything between 1 to 100 Hz. Currently I use one hour of data per document, then there's a 3 dimensional array, first level is minutes, second level is seconds, and third level is samples (double data type). This is inspired by MongoDB for Time Series Data presentations (<a href="https://www.mongodb.com/presentations/mongodb-time-series-data-part-1-setting-stage-sensor-management" rel="nofollow noreferrer">Part 1</a>, <a href="https://www.mongodb.com/presentations/mongodb-time-series-data-part-2-analyzing-time-series-data-using-aggregation-framework" rel="nofollow noreferrer">Part 2</a>).</p>		<p>e.g.</p>		<pre><code>{	  "_id": "2018011200:4", /* Jan 12, 2018 hour 00 UTC for sensor 4 */	  "z": [	    00: [ /* 00h00m */	      00: [ 0.1, 0.0, -0.1, ... ], /* 00h00m00s */	      01: [ 0.1, 0.0, -0.1, ... ], /* 00h00m01s */	      02: [ 0.1, 0.0, -0.1, ... ], /* 00h00m02s */	      ...	      59: [ 0.1, 0.0, -0.1, ... ]  /* 00h00m59s */	    ], ...	  ]	}	</code></pre>		<p>In this way, to get subset of data using <code>$slice</code> can be done only at the minute level, for example if I want to get data from 00:00:00 to 00:00:01, I need to get the whole minute of 00:00 (containing 60 seconds) from MongoDB, then get the second(s) I need in application. Also if I want to get data from 00:00:59 to 00:01:01 then I'll need to get two whole minutes, then in application subset each of them then merge them back. There is a bit of IO waste in this, also some complexity in the app. BTW I have no need to retrieve individual samples, the smallest unit of retrieval (and storage) is a second.</p>		<p>I'm considering a slightly different approach where the hour document is divided directly into array of seconds (as there are 3600 seconds in an hours) and then array of samples. This means to get a data of 5 seconds I will retrieve exactly 5 second of arrays (even if in two different documents, if the time range crosses the hour). There will still be application logic of merging two parts of seconds in different documents, but simpler than the hour/minute/second hierarchy.</p>		<pre><code>{	  "_id": "2018011200:4", /* Jan 12, 2018 hour 00 UTC for sensor 4 */	  "z": [	    0: [ 0.1, 0.0, -0.1, ... ],   /* 00h00m00s */	    1: [ 0.1, 0.0, -0.1, ... ],   /* 00h00m01s */	    2: [ 0.1, 0.0, -0.1, ... ],   /* 00h00m02s */	    ...	    3599: [ 0.1, 0.0, -0.1, ... ] /* 00h59m59s */	  ]	}	</code></pre>		<p>However, I'm also worried that the alternative approach has weaknesses that I'm not aware of. </p>		<p>Which one do you recommend better? What are potential pitfalls that I need to consider? Or perhaps I should consider another design?</p>		<p>Thank you in advance.</p>	"
48237121,83460579,48237121,3,1,Sat Jan 13 10:51:00 EST 2018,1296707,"What is the maximum precision you need? Seconds?"
48237121,83467132,48237121,3,0,Sat Jan 13 17:26:00 EST 2018,122441,"@MarkusWMahlberg each sample is between 10ms (100 Hz) to 1000ms precision, however data is always retrieved in blocks of one second each. For example if sample rate is 40 Hz (typical), there will be 40x60x60=144,000 samples per hour (per channel per station) but these must be retrieved in 40 sample blocks, i.e. can only get 40 or 80 or 120 or 160 or 144,000 samples but cannot get 35 samples or 41 samples."
48237121,48239877,48237121,2,1,Sat Jan 13 12:29:00 EST 2018,1296707,"<p>I think you very much overcomplicated your data model.</p>		<p>Updating a document is much more complicated than simply inserting one. And since your granularity seems to be seconds, we are well within the granularity the <a href="http://bsonspec.org/spec.html" rel="nofollow noreferrer">BSON datatype UTC datetime</a> provides: it is granular to the millisecond.</p>		<p>So as per your data model, assuming that you get a single value per write, simply use something like that:</p>		<pre><code>{	  _id: new ObjectId(),	  value: 0.1,	  sensor: 4,	  ts: new ISODate()	}	</code></pre>		<p>With this data model, we make sure that writes are as cheap as possible without sacrificing information. Then, you can use <a href="https://docs.mongodb.com/manual/core/aggregation-pipeline/" rel="nofollow noreferrer">MongoDB's aggregations</a> to query your data for interesting values. A simple example would be to count the number of values you have for sensor 4 between 2018-01-01T00:00:00.000Z and 2018-01-02T23:59:59.999Z:</p>		<pre><code>db.values.aggregate([	  {"$match":{"sensor":4,"ts":{"$gte":ISODate("2018-01-01"),"$lt":ISODate("2018-01-02")}}},	  {"$sort":{"ts":-1}},	  { "$group": {	      "_id": {	          "year": { "$year": "$ts" },	          "dayOfYear": { "$dayOfYear": "$ts" },	          "hourOfDay": {"$hour":"$ts"},	          "minuteOfHour": {"$minute":"$ts"},	          "secondOfMinute": {	            "$subtract": [ 	              { "$second": "$ts" },	              { "$mod": [{ "$second": "$ts"}, 1] }	            ]	          }	      },	      "count": { $sum: 1 }	    }},	  ],{"allowDiskUse":true})	</code></pre>		<p>Even better, you can use the <a href="https://docs.mongodb.com/manual/reference/operator/aggregation/out/" rel="nofollow noreferrer"><strong><code>$out</code></strong> stage</a> to save your aggregations for faster access.</p>		<p><strong>EDIT:</strong> Please note that you have to make proper use of indexing to make this approach efficient. By itself, even with my rather limited test set of 50M sample documents, the aggregation takes seconds. With indexing, we are talking of around 80 msecs, to give you an impression.</p>	"
48237121,83478477,48239877,3,1,Sun Jan 14 08:49:00 EST 2018,1296707,"@HendyIrawan The preallocation won't help you much, if at all. WT uses COW, so preallocation is futile. The updates are what concerns me. You will have a query, (read), modify, write cycle. Since you are expecting a couple of billion data points/hour, imho you should start with a shared cluster right away (even if it only has one shard). Good luck, anyways! ;)"
48237121,83476352,48239877,3,0,Sun Jan 14 05:06:00 EST 2018,122441,"Unfortunately I do not have the option of using another infrastructure at this point. Regarding your schema, it is interesting but I am expecting around 8 bytes per sample instead of 68 bytes. Yes IOPS will be a problem, currently I plan to preallocate the arrays and only write in at least 1 second blocks (i.e. 40 samples at a time) and up to 1 minute blocks (2400 samples at a time)."
48237121,83467027,48239877,3,0,Sat Jan 13 17:21:00 EST 2018,122441,"Thanks for your suggestion, but I don't think it would be practical. Using my original schema the size of a document would be 2.8 MB per hour per channel per sensor with a sample rate of 40 Hz. Using your approach that'd be **144,000 documents per hour per channel per sensor**. I did a `Object.bsonsize()` of your sample and it is 65 bytes, meaning 9.4 MB per hour per channel per sensor for the document contents alone (not including metadata and indexes). With 1M sensors it means 9.36 TB per hour per channel plus indexes ðŸ˜ž"
48237121,83468862,48239877,3,1,Sat Jan 13 18:53:00 EST 2018,1296707,"@HendyIrawan I have set up a test database, consisting of 50M documents with disk space consumed of 3.2GB, averaging 68 bytes/document. With that much data coming in, you need to [shard](https://docs.mongodb.com/manual/sharding/) anyway. Are those 1M sensors estimates or fixed size? Given your data model, you would most likely need to scale out more just because of the IOPS you'd need."
48237121,83468906,48239877,3,1,Sat Jan 13 18:55:00 EST 2018,1296707,"@HendyIrawan You might instead have a look at the [TICK Stack](https://www.influxdata.com/time-series-platform/) for storing and analyzing the data."
48237121,83482441,48239877,3,0,Sun Jan 14 13:17:00 EST 2018,122441,"Thanks Markus, so it means techniques for MMAP no longer applies to WiredTiger. The MongoDB time series presentations were in 2014, fascinating how best practices change that quickly. It seems I have to choose between storage space vs insert performance at this point.. :("
