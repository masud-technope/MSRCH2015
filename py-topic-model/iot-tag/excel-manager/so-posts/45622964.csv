thread_ID, Id, ParentId, PostType, Score, CreationDate, OwnerID, Body
45622964,45622964,null,1,1,Thu Aug 10 20:30:00 EDT 2017,89303,"<p>I stumbled at a couple of workloads which seem to require filtering data with "lookback" capability - mainly in IoT scenarios, where sensors can produce garbage data, and to detect that it's necessary to look at the previous record of that sensor.</p>		<p>Spark's <code>filter()</code> operation is obviously "element-only" - in fact, the RDD as a whole can't know the order of the elements you want it to look behind on. So another approach is needed.</p>		<p>My naive approach would involve keying the RDD on the sensor, repartitioning it so that keys and partitions become one and the same, and sorting all elements for the keys/partition so that they are in temporal order. Then we can filter with a user function and maybe emit the data back so that the rest of the pipeline can deal with it as it wishes.</p>		<p>However, this looks heavyweight and likely inefficient. Is there a more idiomatic way?</p>		<p><strong>Summary: Is there a Spark-related design pattern to deal with filtering tasks which need to "lookback" at the previous element of a sequence?</strong></p>	"
